# -*- coding: utf-8 -*-
"""MLBM TASK2 ASSIGNMENT 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CfrH1USpGt9l5whZJ0-AIwbQTVGmhiZs
"""

!pip install gradio matplotlib pandas numpy requests

import gradio as gr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
from datetime import datetime, timedelta
import re
from urllib.parse import urlparse
import time

def extract_page_title(url):
    """Extract the page title from a Wikipedia URL."""
    parsed_url = urlparse(url)
    if 'wikipedia.org' not in parsed_url.netloc:
        return None

    path_parts = parsed_url.path.split('/')
    if len(path_parts) >= 3 and path_parts[1] == 'wiki':
        return path_parts[2]
    return None

def get_page_views(page_title, start_date, end_date, language='en', max_retries=3):
    """
    Fetch page view statistics for a Wikipedia page within a date range.

    Args:
        page_title (str): Title of the Wikipedia page.
        start_date (datetime): Start date for data collection.
        end_date (datetime): End date for data collection.
        language (str): Language code for Wikipedia (default: 'en').
        max_retries (int): Maximum number of retry attempts.

    Returns:
        dict: Dictionary with dates as keys and view counts as values.
    """
    # Format dates as required by the API
    start_str = start_date.strftime('%Y%m%d00')
    end_str = end_date.strftime('%Y%m%d00')

    # URL encode the page title (replace spaces with underscores)
    page_title = page_title.replace(' ', '_')

    # Prepare API URL
    url = f"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{language}.wikipedia/all-access/all-agents/{page_title}/daily/{start_str}/{end_str}"

    # Set proper headers to avoid being blocked
    headers = {
        'User-Agent': 'WikiTrendsApp/1.0 (Educational Project; your-email@example.com)',
        'Accept': 'application/json'
    }

    retry_count = 0
    while retry_count < max_retries:
        try:
            response = requests.get(url, headers=headers)

            if response.status_code == 200:
                data = response.json()
                views_dict = {}

                for item in data['items']:
                    date = datetime.strptime(item['timestamp'], '%Y%m%d00')
                    date_str = date.strftime('%Y-%m-%d')
                    views_dict[date_str] = item['views']

                return views_dict

            elif response.status_code == 429:  # Too Many Requests
                retry_count += 1
                wait_time = int(response.headers.get('Retry-After', 10))
                print(f"Rate limited. Waiting {wait_time} seconds before retry {retry_count}/{max_retries}")
                time.sleep(wait_time)
                continue

            elif response.status_code == 403:  # Forbidden
                return {
                    'error': f"API Error: Access forbidden. Wikipedia API has blocked requests. Try using a smaller date range or use a mock dataset for testing purposes."
                }
            else:
                return {
                    'error': f"API Error: {response.status_code} - {response.reason}. Try using a smaller date range or using a mock dataset."
                }

        except Exception as e:
            return {'error': f"Exception: {str(e)}"}

    return {'error': f"Failed after {max_retries} retry attempts. API may be temporarily unavailable."}

def generate_mock_data(start_date, end_date, topic_name, trend='random'):
    """
    Generate mock data for testing when API is unavailable.

    Args:
        start_date (datetime): Start date
        end_date (datetime): End date
        topic_name (str): Name of the topic
        trend (str): Type of trend to generate ('random', 'increasing', 'decreasing', 'spike')

    Returns:
        dict: Dictionary with dates as keys and view counts as values
    """
    views_dict = {}
    days = (end_date - start_date).days + 1

    base_views = np.random.randint(1000, 10000)

    for i in range(days):
        current_date = start_date + timedelta(days=i)
        date_str = current_date.strftime('%Y-%m-%d')

        if trend == 'random':
            views = base_views + np.random.randint(-500, 500)
        elif trend == 'increasing':
            views = base_views + i * (np.random.randint(50, 150))
        elif trend == 'decreasing':
            views = base_views + (days - i) * (np.random.randint(50, 150))
        elif trend == 'spike':
            if i == days // 2:
                views = base_views * 3
            else:
                views = base_views + np.random.randint(-300, 300)

        views_dict[date_str] = max(100, views)  # Ensure views don't go below 100

    return views_dict

def compare_wiki_trends(url1, url2, start_date, end_date, use_mock_data=False):
    """
    Compare Wikipedia page views for two topics over a date range.

    Args:
        url1 (str): First Wikipedia page URL.
        url2 (str): Second Wikipedia page URL.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.
        use_mock_data (bool): Whether to use mock data instead of API calls.

    Returns:
        tuple: (DataFrame of page views, Plot figure, Status message)
    """
    try:
        # Convert string dates to datetime objects
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')

        # Check if date range is valid
        if start_dt >= end_dt:
            return None, None, "Error: Start date must be before end date."

        # Check if date range is too large (to avoid API limits)
        if (end_dt - start_dt).days > 60 and not use_mock_data:
            return None, None, "Error: Date range too large. Please limit to 60 days or less to avoid API limits."

        # Extract page titles from URLs
        page1 = extract_page_title(url1)
        page2 = extract_page_title(url2)

        if not page1 or not page2:
            return None, None, "Error: Invalid Wikipedia URL(s). Please provide URLs in the format: https://en.wikipedia.org/wiki/Page_Title"

        # URL decode the page titles
        page1 = page1.replace('_', ' ')
        page2 = page2.replace('_', ' ')

        # Create display names for the pages (for better labeling)
        display_name1 = page1.replace('_', ' ')
        display_name2 = page2.replace('_', ' ')

        # Fetch page views or use mock data
        if use_mock_data:
            views1 = generate_mock_data(start_dt, end_dt, page1, 'random')
            views2 = generate_mock_data(start_dt, end_dt, page2, 'spike')
            status_prefix = "[USING MOCK DATA] "
        else:
            views1 = get_page_views(page1, start_dt, end_dt)
            views2 = get_page_views(page2, start_dt, end_dt)
            status_prefix = ""

        # Check for errors
        if 'error' in views1:
            # If API error, fall back to mock data
            if "API Error" in views1['error']:
                views1 = generate_mock_data(start_dt, end_dt, page1, 'random')
                status_prefix = f"[USING MOCK DATA for {display_name1}] API Access Error: {views1['error']}. "
            else:
                return None, None, f"Error fetching data for {display_name1}: {views1['error']}"

        if 'error' in views2:
            # If API error, fall back to mock data
            if "API Error" in views2['error']:
                views2 = generate_mock_data(start_dt, end_dt, page2, 'spike')
                status_prefix += f"[USING MOCK DATA for {display_name2}] API Access Error: {views2['error']}. "
            else:
                return None, None, f"Error fetching data for {display_name2}: {views2['error']}"

        # Create DataFrame
        dates = sorted(set(list(views1.keys()) + list(views2.keys())))
        df = pd.DataFrame(index=dates)

        # Add page views to DataFrame
        df[display_name1] = df.index.map(lambda x: views1.get(x, 0))
        df[display_name2] = df.index.map(lambda x: views2.get(x, 0))

        # Reset index to make 'date' a column
        df.reset_index(inplace=True)
        df.rename(columns={'index': 'date'}, inplace=True)

        # Create plot
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(df['date'], df[display_name1], marker='o', label=display_name1)
        ax.plot(df['date'], df[display_name2], marker='x', label=display_name2)

        # Improve plot appearance
        ax.set_title(f'Wikipedia Page Views Comparison: {display_name1} vs {display_name2}')
        ax.set_xlabel('Date')
        ax.set_ylabel('Number of Views')
        ax.legend()
        ax.grid(True)

        # Rotate date labels for better readability
        plt.xticks(rotation=45)
        plt.tight_layout()

        return df, fig, f"{status_prefix}Successfully compared Wikipedia page views for '{display_name1}' and '{display_name2}'."

    except Exception as e:
        return None, None, f"An error occurred: {str(e)}"

# Create Gradio Interface
with gr.Blocks(title="WikiTrends: Compare Wikipedia Page Views") as app:
    gr.Markdown("# ðŸ“Š WikiTrends: Wikipedia Interest Level Tracker")
    gr.Markdown("Compare the number of daily visits to two Wikipedia pages over time to analyze interest trends.")

    with gr.Row():
        with gr.Column():
            url1_input = gr.Textbox(
                label="First Wikipedia URL",
                placeholder="e.g., https://en.wikipedia.org/wiki/Apple_Inc.",
                value="https://en.wikipedia.org/wiki/Apple_Inc.",
                info="Enter the full URL of the first Wikipedia page"
            )
            url2_input = gr.Textbox(
                label="Second Wikipedia URL",
                placeholder="e.g., https://en.wikipedia.org/wiki/Samsung",
                value="https://en.wikipedia.org/wiki/Samsung",
                info="Enter the full URL of the second Wikipedia page"
            )

            with gr.Row():
                start_date = gr.Textbox(
                    label="Start Date",
                    placeholder="YYYY-MM-DD",
                    value=(datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'),
                    info="Start date in YYYY-MM-DD format (recommend keeping within 30 days to avoid API limits)"
                )
                end_date = gr.Textbox(
                    label="End Date",
                    placeholder="YYYY-MM-DD",
                    value=datetime.now().strftime('%Y-%m-%d'),
                    info="End date in YYYY-MM-DD format"
                )

            with gr.Row():
                use_mock_data = gr.Checkbox(
                    label="Use Mock Data",
                    value=False,
                    info="Check this to use generated test data instead of actual Wikipedia API data"
                )
                submit_btn = gr.Button("Compare Trends", variant="primary")

    with gr.Row():
        status_output = gr.Textbox(label="Status")

    with gr.Tabs():
        with gr.TabItem("Graph"):
            plot_output = gr.Plot(label="Interest Comparison Graph")
        with gr.TabItem("Data"):
            data_output = gr.DataFrame(label="Page Views Data")

    # Add example inputs
    gr.Examples(
        examples=[
            ["https://en.wikipedia.org/wiki/Artificial_intelligence", "https://en.wikipedia.org/wiki/Machine_learning", "2023-03-01", "2023-03-10"],
            ["https://en.wikipedia.org/wiki/Python_(programming_language)", "https://en.wikipedia.org/wiki/JavaScript", "2023-02-01", "2023-02-10"],
            ["https://en.wikipedia.org/wiki/OpenAI", "https://en.wikipedia.org/wiki/Google", "2023-01-01", "2023-01-10"]
        ],
        inputs=[url1_input, url2_input, start_date, end_date]
    )

    # Set up the event handler
    submit_btn.click(
        fn=compare_wiki_trends,
        inputs=[url1_input, url2_input, start_date, end_date, use_mock_data],
        outputs=[data_output, plot_output, status_output]
    )

    gr.Markdown("""
    ## How to Use
    1. Enter two Wikipedia page URLs you want to compare
    2. Set the start and end dates for your analysis (keep within 60 days to avoid API limits)
    3. If you encounter API errors, check "Use Mock Data" to test the app functionality
    4. Click "Compare Trends" to analyze the data
    5. View the results as a graph or raw data in the tabs below

    ## Interpretation
    - Higher page views indicate more interest in a topic
    - Spikes often correlate with news events or media coverage
    - Comparing two related topics can reveal interesting patterns in public interest

    ## Troubleshooting
    - If you get API errors, try reducing the date range (< 30 days)
    - Wikipedia may block requests from Google Colab IPs - use mock data option if needed
    - For academic projects, consider using smaller samples of data
    """)

# Launch the app
app.launch()

import gradio as gr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
from datetime import datetime, timedelta
import re
from urllib.parse import urlparse
import time

def extract_page_title(url):
    """Extract the page title from a Wikipedia URL."""
    parsed_url = urlparse(url)
    if 'wikipedia.org' not in parsed_url.netloc:
        return None

    path_parts = parsed_url.path.split('/')
    if len(path_parts) >= 3 and path_parts[1] == 'wiki':
        return path_parts[2]
    return None

def get_page_views(page_title, start_date, end_date, language='en', max_retries=3):
    """
    Fetch page view statistics for a Wikipedia page within a date range.

    Args:
        page_title (str): Title of the Wikipedia page.
        start_date (datetime): Start date for data collection.
        end_date (datetime): End date for data collection.
        language (str): Language code for Wikipedia (default: 'en').
        max_retries (int): Maximum number of retry attempts.

    Returns:
        dict: Dictionary with dates as keys and view counts as values.
    """
    # Format dates as required by the API
    start_str = start_date.strftime('%Y%m%d00')
    end_str = end_date.strftime('%Y%m%d00')

    # URL encode the page title (replace spaces with underscores)
    page_title = page_title.replace(' ', '_')

    # Prepare API URL
    url = f"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{language}.wikipedia/all-access/all-agents/{page_title}/daily/{start_str}/{end_str}"

    # Set proper headers to avoid being blocked
    headers = {
        'User-Agent': 'WikiTrendsResearchProject/1.0 (Educational Project; contactme@example.edu)',
        'Accept': 'application/json',
        'Accept-Encoding': 'gzip'
    }

    retry_count = 0
    while retry_count < max_retries:
        try:
            response = requests.get(url, headers=headers)

            if response.status_code == 200:
                data = response.json()
                views_dict = {}

                for item in data['items']:
                    date = datetime.strptime(item['timestamp'], '%Y%m%d00')
                    date_str = date.strftime('%Y-%m-%d')
                    views_dict[date_str] = item['views']

                return views_dict

            elif response.status_code == 429:  # Too Many Requests
                retry_count += 1
                wait_time = int(response.headers.get('Retry-After', 10))
                print(f"Rate limited. Waiting {wait_time} seconds before retry {retry_count}/{max_retries}")
                time.sleep(wait_time)
                continue

            else:
                return {
                    'error': f"API Error: {response.status_code} - {response.reason}. Try using a smaller date range."
                }

        except Exception as e:
            return {'error': f"Exception: {str(e)}"}

    return {'error': f"Failed after {max_retries} retry attempts. API may be temporarily unavailable."}

def compare_wiki_trends(url1, url2, start_date, end_date):
    """
    Compare Wikipedia page views for two topics over a date range.

    Args:
        url1 (str): First Wikipedia page URL.
        url2 (str): Second Wikipedia page URL.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.

    Returns:
        tuple: (DataFrame of page views, Plot figure, Status message)
    """
    try:
        # Convert string dates to datetime objects
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')

        # Check if date range is valid
        if start_dt >= end_dt:
            return None, None, "Error: Start date must be before end date."

        # Check if date range is too large (to avoid API limits)
        if (end_dt - start_dt).days > 60:
            return None, None, "Error: Date range too large. Please limit to 60 days or less to avoid API limits."

        # Extract page titles from URLs
        page1 = extract_page_title(url1)
        page2 = extract_page_title(url2)

        if not page1 or not page2:
            return None, None, "Error: Invalid Wikipedia URL(s). Please provide URLs in the format: https://en.wikipedia.org/wiki/Page_Title"

        # URL decode the page titles
        page1 = page1.replace('_', ' ')
        page2 = page2.replace('_', ' ')

        # Create display names for the pages (for better labeling)
        display_name1 = page1.replace('_', ' ')
        display_name2 = page2.replace('_', ' ')

        # Fetch page views
        views1 = get_page_views(page1, start_dt, end_dt)
        views2 = get_page_views(page2, start_dt, end_dt)

        # Check for errors
        if 'error' in views1:
            return None, None, f"Error fetching data for {display_name1}: {views1['error']}"

        if 'error' in views2:
            return None, None, f"Error fetching data for {display_name2}: {views2['error']}"

        # Create DataFrame
        dates = sorted(set(list(views1.keys()) + list(views2.keys())))
        df = pd.DataFrame(index=dates)

        # Add page views to DataFrame
        df[display_name1] = df.index.map(lambda x: views1.get(x, 0))
        df[display_name2] = df.index.map(lambda x: views2.get(x, 0))

        # Reset index to make 'date' a column
        df.reset_index(inplace=True)
        df.rename(columns={'index': 'date'}, inplace=True)

        # Create plot
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(df['date'], df[display_name1], marker='o', label=display_name1)
        ax.plot(df['date'], df[display_name2], marker='x', label=display_name2)

        # Improve plot appearance
        ax.set_title(f'Wikipedia Page Views Comparison: {display_name1} vs {display_name2}')
        ax.set_xlabel('Date')
        ax.set_ylabel('Number of Views')
        ax.legend()
        ax.grid(True)

        # Rotate date labels for better readability
        plt.xticks(rotation=45)
        plt.tight_layout()

        return df, fig, f"Successfully compared Wikipedia page views for '{display_name1}' and '{display_name2}'."

    except Exception as e:
        return None, None, f"An error occurred: {str(e)}"

# Create Gradio Interface
with gr.Blocks(title="WikiTrends: Compare Wikipedia Page Views") as app:
    gr.Markdown("# ðŸ“Š WikiTrends: Wikipedia Interest Level Tracker")
    gr.Markdown("Compare the number of daily visits to two Wikipedia pages over time to analyze interest trends.")

    with gr.Row():
        with gr.Column():
            url1_input = gr.Textbox(
                label="First Wikipedia URL",
                value="https://en.wikipedia.org/wiki/Python_(programming_language)",
                info="Enter the full URL of the first Wikipedia page"
            )
            url2_input = gr.Textbox(
                label="Second Wikipedia URL",
                value="https://en.wikipedia.org/wiki/JavaScript",
                info="Enter the full URL of the second Wikipedia page"
            )

            with gr.Row():
                start_date = gr.Textbox(
                    label="Start Date",
                    value="2023-03-01",
                    info="Start date in YYYY-MM-DD format (recommend keeping within 30 days to avoid API limits)"
                )
                end_date = gr.Textbox(
                    label="End Date",
                    value="2023-03-15",
                    info="End date in YYYY-MM-DD format"
                )

            submit_btn = gr.Button("Compare Trends", variant="primary")

    with gr.Row():
        status_output = gr.Textbox(label="Status")

    with gr.Tabs():
        with gr.TabItem("Graph"):
            plot_output = gr.Plot(label="Interest Comparison Graph")
        with gr.TabItem("Data"):
            data_output = gr.DataFrame(label="Page Views Data")

    # Add example inputs
    gr.Examples(
        examples=[
            ["https://en.wikipedia.org/wiki/Python_(programming_language)", "https://en.wikipedia.org/wiki/JavaScript", "2023-03-01", "2023-03-15"],
            ["https://en.wikipedia.org/wiki/Linux", "https://en.wikipedia.org/wiki/Windows", "2023-02-01", "2023-02-15"],
            ["https://en.wikipedia.org/wiki/Climate_change", "https://en.wikipedia.org/wiki/Global_warming", "2023-01-01", "2023-01-15"]
        ],
        inputs=[url1_input, url2_input, start_date, end_date]
    )

    # Set up the event handler
    submit_btn.click(
        fn=compare_wiki_trends,
        inputs=[url1_input, url2_input, start_date, end_date],
        outputs=[data_output, plot_output, status_output]
    )

    gr.Markdown("""
    ## How to Use
    1. Enter two Wikipedia page URLs you want to compare (less popular pages work better with API limits)
    2. Set the start and end dates for your analysis (keep within 14 days to avoid API limits)
    3. Click "Compare Trends" to analyze the data
    4. View the results as a graph or raw data in the tabs below

    ## Interpretation
    - Higher page views indicate more interest in a topic
    - Spikes often correlate with news events or media coverage
    - Comparing two related topics can reveal interesting patterns in public interest

    ## Troubleshooting
    - If you get API errors, try reducing the date range (< 14 days)
    - Try using less popular Wikipedia pages (not major companies or celebrities)
    - Make sure you're using full Wikipedia URLs with the /wiki/ path
    """)

# Launch the app
app.launch()

import gradio as gr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
from datetime import datetime, timedelta
import re
from urllib.parse import urlparse
import time

def extract_page_title(url):
    """Extract the page title from a Wikipedia URL."""
    parsed_url = urlparse(url)
    if 'wikipedia.org' not in parsed_url.netloc:
        return None

    path_parts = parsed_url.path.split('/')
    if len(path_parts) >= 3 and path_parts[1] == 'wiki':
        return path_parts[2]
    return None

def get_page_views(page_title, start_date, end_date, language='en', max_retries=3):
    """
    Fetch page view statistics for a Wikipedia page within a date range.

    Args:
        page_title (str): Title of the Wikipedia page.
        start_date (datetime): Start date for data collection.
        end_date (datetime): End date for data collection.
        language (str): Language code for Wikipedia (default: 'en').
        max_retries (int): Maximum number of retry attempts.

    Returns:
        dict: Dictionary with dates as keys and view counts as values.
    """
    # Format dates as required by the API
    start_str = start_date.strftime('%Y%m%d00')
    end_str = end_date.strftime('%Y%m%d00')

    # URL encode the page title (replace spaces with underscores)
    page_title = page_title.replace(' ', '_')

    # Prepare API URL
    url = f"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{language}.wikipedia/all-access/all-agents/{page_title}/daily/{start_str}/{end_str}"

    # Set proper headers to avoid being blocked
    headers = {
        'User-Agent': 'WikiTrendsResearchProject/1.0 (Educational Project; contactme@example.edu)',
        'Accept': 'application/json',
        'Accept-Encoding': 'gzip'
    }

    retry_count = 0
    while retry_count < max_retries:
        try:
            response = requests.get(url, headers=headers)

            if response.status_code == 200:
                data = response.json()
                views_dict = {}

                for item in data['items']:
                    date = datetime.strptime(item['timestamp'], '%Y%m%d00')
                    date_str = date.strftime('%Y-%m-%d')
                    views_dict[date_str] = item['views']

                return views_dict

            elif response.status_code == 429:  # Too Many Requests
                retry_count += 1
                wait_time = int(response.headers.get('Retry-After', 10))
                print(f"Rate limited. Waiting {wait_time} seconds before retry {retry_count}/{max_retries}")
                time.sleep(wait_time)
                continue

            else:
                return {
                    'error': f"API Error: {response.status_code} - {response.reason}. Try using a smaller date range."
                }

        except Exception as e:
            return {'error': f"Exception: {str(e)}"}

    return {'error': f"Failed after {max_retries} retry attempts. API may be temporarily unavailable."}

def compare_wiki_trends(url1, url2, start_date, end_date):
    """
    Compare Wikipedia page views for two topics over a date range.

    Args:
        url1 (str): First Wikipedia page URL.
        url2 (str): Second Wikipedia page URL.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.

    Returns:
        tuple: (DataFrame of page views, Plot figure, Status message)
    """
    try:
        # Convert string dates to datetime objects
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')

        # Check if date range is valid
        if start_dt >= end_dt:
            return None, None, "Error: Start date must be before end date."

        # Check if date range is too large (to avoid API limits)
        if (end_dt - start_dt).days > 60:
            return None, None, "Error: Date range too large. Please limit to 60 days or less to avoid API limits."

        # Extract page titles from URLs
        page1 = extract_page_title(url1)
        page2 = extract_page_title(url2)

        if not page1 or not page2:
            return None, None, "Error: Invalid Wikipedia URL(s). Please provide URLs in the format: https://en.wikipedia.org/wiki/Page_Title"

        # URL decode the page titles
        page1 = page1.replace('_', ' ')
        page2 = page2.replace('_', ' ')

        # Create display names for the pages (for better labeling)
        display_name1 = page1.replace('_', ' ')
        display_name2 = page2.replace('_', ' ')

        # Fetch page views
        views1 = get_page_views(page1, start_dt, end_dt)
        views2 = get_page_views(page2, start_dt, end_dt)

        # Check for errors
        if 'error' in views1:
            return None, None, f"Error fetching data for {display_name1}: {views1['error']}"

        if 'error' in views2:
            return None, None, f"Error fetching data for {display_name2}: {views2['error']}"

        # Create DataFrame
        dates = sorted(set(list(views1.keys()) + list(views2.keys())))
        df = pd.DataFrame(index=dates)

        # Add page views to DataFrame
        df[display_name1] = df.index.map(lambda x: views1.get(x, 0))
        df[display_name2] = df.index.map(lambda x: views2.get(x, 0))

        # Reset index to make 'date' a column
        df.reset_index(inplace=True)
        df.rename(columns={'index': 'date'}, inplace=True)

        # Create plot
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(df['date'], df[display_name1], marker='o', label=display_name1)
        ax.plot(df['date'], df[display_name2], marker='x', label=display_name2)

        # Improve plot appearance
        ax.set_title(f'Wikipedia Page Views Comparison: {display_name1} vs {display_name2}')
        ax.set_xlabel('Date')
        ax.set_ylabel('Number of Views')
        ax.legend()
        ax.grid(True)

        # Rotate date labels for better readability
        plt.xticks(rotation=45)
        plt.tight_layout()

        return df, fig, f"Successfully compared Wikipedia page views for '{display_name1}' and '{display_name2}'."

    except Exception as e:
        return None, None, f"An error occurred: {str(e)}"

# Create Gradio Interface
with gr.Blocks(title="WikiTrends: Compare Wikipedia Page Views") as app:
    gr.Markdown("# ðŸ“Š WikiTrends: Wikipedia Interest Level Tracker")
    gr.Markdown("Compare the number of daily visits to two Wikipedia pages over time to analyze interest trends.")

    with gr.Row():
        with gr.Column():
            url1_input = gr.Textbox(
                label="First Wikipedia URL",
                placeholder="https://en.wikipedia.org/wiki/Your_Topic",
                info="Enter the full URL of the first Wikipedia page"
            )
            url2_input = gr.Textbox(
                label="Second Wikipedia URL",
                placeholder="https://en.wikipedia.org/wiki/Another_Topic",
                info="Enter the full URL of the second Wikipedia page"
            )

            with gr.Row():
                start_date = gr.Textbox(
                    label="Start Date",
                    placeholder="YYYY-MM-DD",
                    info="Start date in YYYY-MM-DD format (recommend keeping within 30 days to avoid API limits)"
                )
                end_date = gr.Textbox(
                    label="End Date",
                    placeholder="YYYY-MM-DD",
                    info="End date in YYYY-MM-DD format"
                )

            submit_btn = gr.Button("Compare Trends", variant="primary")

    with gr.Row():
        status_output = gr.Textbox(label="Status")

    with gr.Tabs():
        with gr.TabItem("Graph"):
            plot_output = gr.Plot(label="Interest Comparison Graph")
        with gr.TabItem("Data"):
            data_output = gr.DataFrame(label="Page Views Data")

    # Add example inputs
    gr.Examples(
        examples=[
            ["https://en.wikipedia.org/wiki/Python_(programming_language)", "https://en.wikipedia.org/wiki/JavaScript", "2023-03-01", "2023-03-15"],
            ["https://en.wikipedia.org/wiki/Linux", "https://en.wikipedia.org/wiki/Windows", "2023-02-01", "2023-02-15"],
            ["https://en.wikipedia.org/wiki/Climate_change", "https://en.wikipedia.org/wiki/Global_warming", "2023-01-01", "2023-01-15"],
            ["https://en.wikipedia.org/wiki/Artificial_intelligence", "https://en.wikipedia.org/wiki/Machine_learning", "2024-01-01", "2024-01-30"]
        ],
        inputs=[url1_input, url2_input, start_date, end_date]
    )

    # Set up the event handler
    submit_btn.click(
        fn=compare_wiki_trends,
        inputs=[url1_input, url2_input, start_date, end_date],
        outputs=[data_output, plot_output, status_output]
    )

    gr.Markdown("""
    ## How to Use
    1. Enter two Wikipedia page URLs you want to compare
    2. Set the start and end dates for your analysis (keeping within 14-30 days is recommended to avoid API limits)
    3. Click "Compare Trends" to analyze the data
    4. View the results as a graph or raw data in the tabs below

    ## Interpretation
    - Higher page views indicate more interest in a topic
    - Spikes often correlate with news events or media coverage
    - Comparing two related topics can reveal interesting patterns in public interest

    ## Troubleshooting
    - If you get API errors, try reducing the date range (< 14 days)
    - Try using less popular Wikipedia pages (not major companies or celebrities)
    - Make sure you're using full Wikipedia URLs with the /wiki/ path
    - The Wikimedia API has rate limits, so you may need to wait a bit between requests
    """)

# Launch the app
if __name__ == "__main__":
    app.launch()

